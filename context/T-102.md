## T-102 — Interfaz común de cliente LLM (para agentes) — **EXPANDIDO (implementable)**

**Objetivo:** desacoplar los agentes del proveedor de IA, estandarizar **entradas/salidas**, manejo de errores, retries, trazabilidad y mocks determinísticos para tests.

> **Decisión cerrada:** Los agentes NO llaman SDKs directamente. Solo usan `LLMClient`.  
> **Decisión cerrada:** El cliente expone **dos rutas**: texto libre y JSON validado por schema.

---

### T-102.0 — Alcance y no-alcance

**Incluye**
- Contratos de datos (Request/Response) para texto y JSON.
- Validación de JSON contra schema (en cliente).
- Retries con backoff, timeouts, y errores tipados.
- Registro de trazas mínimas (latencia, tokens, request_id).
- Implementación concreta `OpenAIClient` (configurable) **sin acoplar agentes**.
- `MockLLMClient` determinístico para tests y replay.

**NO incluye**
- Persistencia de conversación larga / memoria (eso vive en Step Context).
- Prompt engineering específico de cada agente (eso vive en los agentes).
- Streaming (por ahora sync; streaming puede ser CR).

---

### T-102.1 — Contratos de datos (interfaces y modelos)

#### A) Mensajes (formato estándar)
Los agentes generan una lista de mensajes tipo chat:

```python
from dataclasses import dataclass
from typing import Literal, Optional, Any, Dict, List

Role = Literal["system", "user", "assistant"]

@dataclass
class LLMMessage:
    role: Role
    content: str
```

#### B) Request base
```python
@dataclass
class LLMRequest:
    messages: List[LLMMessage]
    model: str
    temperature: float = 0.2
    max_tokens: int = 1200
    top_p: Optional[float] = None
    seed: Optional[int] = None  # si el proveedor lo soporta
    timeout_s: float = 60.0
    # Metadata para trazabilidad (no se manda al modelo salvo que el provider lo permita)
    run_id: Optional[str] = None
    step_name: Optional[str] = None
    beat_id: Optional[str] = None
    tags: Optional[Dict[str, str]] = None
```

#### C) JSON request (con schema)
`generate_json` exige un schema (dict) que describe el objeto esperado.

```python
@dataclass
class LLMJsonRequest(LLMRequest):
    json_schema: Dict[str, Any] = None  # JSON Schema-like (mínimo requerido)
```

> **Regla:** si `json_schema` es None o vacío → error `INVALID_SCHEMA`.

#### D) Response estándar
```python
@dataclass
class LLMUsage:
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0

@dataclass
class LLMResponse:
    text: str
    # Solo para generate_json (o si el provider devuelve json)
    json: Optional[Dict[str, Any]] = None
    usage: Optional[LLMUsage] = None
    request_id: Optional[str] = None
    provider: Optional[str] = None
    model: Optional[str] = None
    latency_ms: Optional[int] = None
    raw: Optional[Dict[str, Any]] = None  # respuesta original del proveedor (opcional)
```

---

### T-102.2 — API del cliente (lo que los agentes pueden usar)

```python
from abc import ABC, abstractmethod

class LLMClient(ABC):
    @abstractmethod
    def generate_text(self, req: LLMRequest) -> LLMResponse:
        ...

    @abstractmethod
    def generate_json(self, req: LLMJsonRequest) -> LLMResponse:
        ...
```

**Reglas de uso**
- Agentes SOLO pueden llamar `generate_text` o `generate_json`.
- Agentes NO pueden:
  - importar SDKs del proveedor
  - hacer requests HTTP directas
  - manejar retries/ratelimits (eso es responsabilidad del cliente)

---

### T-102.3 — Validación y parsing de JSON (enforcement)

#### Requerimiento obligatorio
`generate_json` debe:
1) pedir al proveedor una salida estrictamente JSON (como se logre con ese proveedor), y
2) **parsear** la respuesta a dict, y
3) **validarla** contra `json_schema` antes de devolverla.

#### Especificación mínima de validación
- Validar que el resultado sea un `dict`.
- Validar campos requeridos (ej. `required`).
- Validar tipos básicos (`string`, `number`, `integer`, `boolean`, `array`, `object`).
- Validar enums si el schema los trae (`enum`).

> Implementación recomendada: `jsonschema` (lib) o validación propia mínima.  
> **Regla:** si la validación falla → error `JSON_SCHEMA_VIOLATION` (sin “silenciar”).

---

### T-102.4 — Manejo de errores y retries

#### Errores tipados (obligatorio)
Crear excepciones:

```python
class LLMError(Exception): ...
class LLMTimeoutError(LLMError): ...
class LLMRateLimitError(LLMError): ...
class LLMProviderError(LLMError): ...
class LLMInvalidSchemaError(LLMError): ...
class LLMJsonParseError(LLMError): ...
class LLMJsonSchemaViolationError(LLMError): ...
```

#### Retry policy (obligatoria)
- Retries solo para:
  - rate limit
  - errores transitorios de red/proveedor
  - timeouts (1 retry máximo)
- No reintentar para:
  - schema inválido
  - json parse / schema violation (a menos que se habilite modo `repair_json` por CR)

**Parámetros**
- `max_retries`: 3 (rate limit/transient)
- `backoff`: exponencial con jitter
- `timeout_s`: configurable por request

---

### T-102.5 — Logging + trazabilidad (por RUN / step / beat)

#### Requerimientos
Cada request debe registrar (a nivel INFO o DEBUG según setting):
- `run_id`, `step_name`, `beat_id`
- `provider`, `model`
- `latency_ms`
- `usage` (tokens)
- `request_id` (si lo provee el proveedor)
- `error_code` y mensaje si falla

#### Redacción (obligatorio)
- No loggear keys ni tokens
- No loggear prompts completos en INFO (solo en DEBUG o guardarlos en artefactos controlados)

---

### T-102.6 — Configuración del provider (OpenAIClient configurable)

#### Requerimientos mínimos (sin hardcode)
- Variables de entorno:
  - `LLM_PROVIDER` (ej. `openai`)
  - `OPENAI_API_KEY`
  - `OPENAI_BASE_URL` (opcional)
- Selección de provider desde `system_rules.yaml -> agent_settings.*`

#### Clase concreta
- `OpenAIClient( ... )` implementa `LLMClient`
- Debe aceptar:
  - `api_key`
  - `base_url` (opcional)
  - `default_timeout_s`
  - `max_retries`
- Debe mapear:
  - `LLMRequest.messages` → formato chat del proveedor
  - `temperature/max_tokens/seed` → si el proveedor lo soporta

> Nota: la implementación interna puede usar SDK oficial o HTTP. **Pero** hacia afuera debe respetar la interfaz `LLMClient`.

---

### T-102.7 — MockLLMClient (determinístico para tests)

#### Objetivo
Tests sin red y resultados repetibles.

#### Diseño requerido
`MockLLMClient` soporta:
- Modo “mapping”:
  - key = hash estable de (`model + messages + json_schema(optional)`)
  - value = respuesta fixture (text o json)
- Modo “regex” (opcional):
  - si prompt matchea patrón, devolver fixture

Ejemplo:
```python
class MockLLMClient(LLMClient):
    def __init__(self, fixtures: dict): ...
    def generate_text(self, req): ...
    def generate_json(self, req): ...
```

#### AC
- Si no hay fixture para una request → error claro `MISSING_FIXTURE` (para no esconder fallas).
- Fixtures viven en `tests/fixtures/llm/`.

---

### T-102.8 — Factory / Provider Router

#### Requerimiento
Crear `LLMClientFactory`:

```python
def build_llm_client(provider: str, settings: dict) -> LLMClient:
    ...
```

- `provider` sale de config (`agent_settings.*.provider`)
- `settings` incluye credenciales/timeouts/retries

---

### T-102.9 — Integración obligatoria en agentes

#### Reglas
- `BeatSegmenterAgent`, `VisualPlannerAgent`, `PromptBuilderAgent` reciben `llm: LLMClient` por DI (dependency injection).
- En tests, se pasa `MockLLMClient`.

---

### T-102.10 — Criterios de Aceptación (AC)
1) Existe `LLMClient` con `generate_text` y `generate_json`.
2) `generate_json` valida contra schema y falla con error tipado si no cumple.
3) Retries/backoff implementados con límites definidos.
4) `OpenAIClient` funciona (smoke test) pero los tests CI usan `MockLLMClient`.
5) Agentes NO importan SDKs; solo usan `LLMClient`.
6) Logging incluye `run_id/step/beat`, latencia, usage y request_id.

---

### T-102.11 — Tests obligatorios
- Unit: `test_generate_json_validates_schema_pass`
- Unit: `test_generate_json_schema_violation_raises`
- Unit: `test_retry_on_rate_limit`
- Unit: `test_no_retry_on_schema_invalid`
- Unit: `test_mock_llm_missing_fixture_fails_loudly`
- Integration: `BeatSegmenterAgent` con `MockLLMClient` produce output determinístico

---
